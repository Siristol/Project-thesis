{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29905104",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-20 13:43:09.299823: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-10-20 13:43:09.301573: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-20 13:43:09.475215: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-20 13:43:10.927326: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-20 13:43:10.928854: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow 2.20.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "print(f\"Tensorflow {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94279400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load the CIFAR-10 dataset\n",
    "import pickle\n",
    "from keras.utils import to_categorical\n",
    "def unpickle(file):\n",
    "    \"\"\"load the cifar-10 data\"\"\"\n",
    "\n",
    "    with open(file, 'rb') as fo:\n",
    "        data = pickle.load(fo, encoding='bytes')\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_cifar_10_data(data_dir, negatives=False):\n",
    "    \"\"\"\n",
    "    Return train_data, train_filenames, train_labels, test_data, test_filenames, test_labels\n",
    "    \"\"\"\n",
    "\n",
    "    # get the meta_data_dict\n",
    "    # num_cases_per_batch: 1000\n",
    "    # label_names: ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "    # num_vis: :3072\n",
    "\n",
    "    meta_data_dict = unpickle(data_dir + \"/batches.meta\")\n",
    "    cifar_label_names = meta_data_dict[b'label_names']\n",
    "    cifar_label_names = np.array(cifar_label_names)\n",
    "\n",
    "    # training data\n",
    "    cifar_train_data = None\n",
    "    cifar_train_filenames = []\n",
    "    cifar_train_labels = []\n",
    "\n",
    "    for i in range(1, 6):\n",
    "        cifar_train_data_dict = unpickle(data_dir + \"/data_batch_{}\".format(i))\n",
    "        if i == 1:\n",
    "            cifar_train_data = cifar_train_data_dict[b'data']\n",
    "        else:\n",
    "            cifar_train_data = np.vstack((cifar_train_data, cifar_train_data_dict[b'data']))\n",
    "        cifar_train_filenames += cifar_train_data_dict[b'filenames']\n",
    "        cifar_train_labels += cifar_train_data_dict[b'labels']\n",
    "\n",
    "    cifar_train_data = cifar_train_data.reshape((len(cifar_train_data), 3, 32, 32))\n",
    "    if negatives:\n",
    "        cifar_train_data = cifar_train_data.transpose(0, 2, 3, 1).astype(np.float32)\n",
    "    else:\n",
    "        cifar_train_data = np.rollaxis(cifar_train_data, 1, 4)\n",
    "    cifar_train_filenames = np.array(cifar_train_filenames)\n",
    "    cifar_train_labels = np.array(cifar_train_labels)\n",
    "\n",
    "    cifar_test_data_dict = unpickle(data_dir + \"/test_batch\")\n",
    "    cifar_test_data = cifar_test_data_dict[b'data']\n",
    "    cifar_test_filenames = cifar_test_data_dict[b'filenames']\n",
    "    cifar_test_labels = cifar_test_data_dict[b'labels']\n",
    "\n",
    "    cifar_test_data = cifar_test_data.reshape((len(cifar_test_data), 3, 32, 32))\n",
    "    if negatives:\n",
    "        cifar_test_data = cifar_test_data.transpose(0, 2, 3, 1).astype(np.float32)\n",
    "    else:\n",
    "        cifar_test_data = np.rollaxis(cifar_test_data, 1, 4)\n",
    "    cifar_test_filenames = np.array(cifar_test_filenames)\n",
    "    cifar_test_labels = np.array(cifar_test_labels)\n",
    "\n",
    "    return cifar_train_data, cifar_train_filenames, to_categorical(cifar_train_labels), \\\n",
    "        cifar_test_data, cifar_test_filenames, to_categorical(cifar_test_labels), cifar_label_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de9c0799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANNarchy 4.8 (4.8.2.5) on linux (posix).\n"
     ]
    }
   ],
   "source": [
    "import ANNarchy\n",
    "from ANNarchy.extensions.ann_to_snn_conversion import ANNtoSNNConverter\n",
    "ANNarchy.clear()\n",
    "snn_converter = ANNtoSNNConverter(\n",
    "    input_encoding='IB', \n",
    "    hidden_neuron='IaF',\n",
    "    read_out='spike_count',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebe02fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sist/projects/test_ann_to_snn/venv_annarchy/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:86: UserWarning: Argument `decay` is no longer supported and will be ignored.\n",
      "  warnings.warn(\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "# Load the pretrained Keras model\n",
    "model = tf.keras.models.load_model('pretrainedResnet.h5')\n",
    "model.save('pretrainedResnet.keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b0c2652",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sist/projects/test_ann_to_snn/venv_annarchy/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:797: UserWarning: Skipping variable loading for optimizer 'Adam', because it has 70 variables whereas the saved optimizer has 2 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Dense representation is an experimental feature for spiking models, we greatly appreciate bug reports. \n",
      "* Input layer: input_1, (32, 32, 3)\n",
      "* InputLayer skipped.\n",
      "* Conv2D layer: conv2d, (32, 32, 16) \n",
      "* BatchNormalization skipped.\n",
      "* Activation skipped.\n",
      "* Conv2D layer: conv2d_1, (32, 32, 16) \n",
      "* BatchNormalization skipped.\n",
      "* Activation skipped.\n",
      "* Conv2D layer: conv2d_2, (32, 32, 16) \n",
      "* BatchNormalization skipped.\n",
      "* Add skipped.\n",
      "* Activation skipped.\n",
      "* Conv2D layer: conv2d_3, (16, 16, 32) \n",
      "* BatchNormalization skipped.\n",
      "* Activation skipped.\n",
      "* Conv2D layer: conv2d_4, (16, 16, 32) \n",
      "* Conv2D layer: conv2d_5, (16, 16, 32) \n",
      "* BatchNormalization skipped.\n",
      "* Add skipped.\n",
      "* Activation skipped.\n",
      "* Conv2D layer: conv2d_6, (8, 8, 64) \n",
      "* BatchNormalization skipped.\n",
      "* Activation skipped.\n",
      "* Conv2D layer: conv2d_7, (8, 8, 64) \n",
      "* Conv2D layer: conv2d_8, (8, 8, 64) \n",
      "* BatchNormalization skipped.\n",
      "* Add skipped.\n",
      "* Activation skipped.\n",
      "* MaxPooling2D layer: average_pooling2d, (1, 1, 64) \n",
      "* Flatten skipped.\n",
      "* Dense layer: dense, 10 \n",
      "    weights: (10, 64)\n",
      "    mean -0.16150876879692078, std 1.1581709384918213\n",
      "    min -2.7678143978118896, max 3.8804078102111816\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net = snn_converter.load_keras_model(\"pretrainedResnet.keras\", show_info=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf19a3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_filenames, train_labels, test_data, test_filenames, test_labels, label_names = load_cifar_10_data('cifar-10-batches-py')\n",
    "# Download data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8111323",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:51<00:00,  1.11s/it]\n"
     ]
    }
   ],
   "source": [
    "predictions_snn = snn_converter.predict(test_data[:100], duration_per_sample=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87144751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.13      0.20      0.16        10\n",
      "           1       0.17      0.17      0.17         6\n",
      "           2       0.00      0.00      0.00         8\n",
      "           3       0.20      0.20      0.20        10\n",
      "           4       0.00      0.00      0.00         7\n",
      "           5       0.13      0.25      0.17         8\n",
      "           6       0.22      0.12      0.16        16\n",
      "           7       0.00      0.00      0.00        11\n",
      "           8       0.18      0.15      0.17        13\n",
      "           9       0.17      0.09      0.12        11\n",
      "\n",
      "    accuracy                           0.12       100\n",
      "   macro avg       0.12      0.12      0.11       100\n",
      "weighted avg       0.13      0.12      0.12       100\n",
      "\n",
      "Test accuracy of the SNN: 0.12\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Convert one-hot labels (returned by load_cifar_10_data) to integer class indices\n",
    "y_true = test_labels[:100]\n",
    "if y_true.ndim == 2 and y_true.shape[1] > 1:\n",
    "    y_true = np.argmax(y_true, axis=1)\n",
    "\n",
    "print(classification_report(y_true, predictions_snn))\n",
    "print(\"Test accuracy of the SNN:\", accuracy_score(y_true, predictions_snn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1253439c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.8719 - loss: 0.4542\n",
      "Accuracy keras:  0.8719000220298767\n"
     ]
    }
   ],
   "source": [
    "# Recompile the model to fix any potential metrics compilation issues\n",
    "model.compile(\n",
    "    optimizer='sgd',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "test_metrics = model.evaluate(x=test_data, y=test_labels, batch_size=50, verbose=1, return_dict=True)\n",
    "print(\"Accuracy keras: \", test_metrics['accuracy'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_annarchy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
